# **LLaMA**
![](llama.png)


### **Papers**  
I am reading these papers:
- [ ] [LLaMA: Open and Efficient Foundation Language Models](https://ai.meta.com/research/publications/llama-open-and-efficient-foundation-language-models/)
- [ ] [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)
- [ ] [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [ ] [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068)


### **Articles**
- [ ] [Understanding SentencePiece ([Under][Standing][_Sentence][Piece])](https://colabdoge.medium.com/understanding-sentencepiece-under-standing-sentence-piece-ac8da59f6b08)


### **Goals**
- ✅ Understand the concept of dot product of two matrices. 
- ✅ Understand the concept of autoregressive language models.
- ✅ Understand the concept of attention computation.
- ✅ Understand the workings of `Byte-Pair Encoding` (BPE) algorithm and tokenizer. 
- ✅ Read the workings of the `SentencePiece` library and tokenizer.
- ✅ Understand the concept of tokenization, input ids and embedding vectors.
- ✅ Understand the concept of positional encoding.
- ✅ Understand the concept of single head self-attention.
- ✅ Understand the concept of scaled dot-product attention.
- ✅ Understand the concept of multi-head attention.
- ✅ Understand the concept of layer normalization.
- ✅ Understand the concept of masked multi-head attention & softmax layer.
- [ ] Understand and implement the rotatory positional encoding.
- [ ] Understand and implement grouped query attention from scratch.
- [ ] Understand and implement the concept of KV cache.
- [ ] Adding ...


### **Related GitHub Works**
- [pytorch-llama](https://github.com/hkproj/pytorch-llama/tree/main) - PyTorch implementation of LLaMA by Umar Jamil.
- [llama](https://github.com/facebookresearch/llama) - Facebook's LLaMA implementation.