# **LLaMA**
![](llama.png)


### **PapersğŸ“„**  
I am reading these papers:  
â˜‘ï¸ [LLaMA: Open and Efficient Foundation Language Models](https://ai.meta.com/research/publications/llama-open-and-efficient-foundation-language-models/)  
â˜‘ï¸ [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)  
â˜‘ï¸ [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068)  
âœ… [Attention Is All You Need](https://arxiv.org/abs/1706.03762)  
âœ… [Root Mean Square Layer Normalization](https://arxiv.org/abs/1910.07467)  
âœ… [GLU Variants Improve Transformer](https://arxiv.org/abs/2002.05202)  
â˜‘ï¸ [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)  
âœ… [Self-Attention with Relative Position Representations](https://arxiv.org/pdf/1803.02155.pdf)  
â˜‘ï¸ [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)


### **Goals ğŸš€**
âœ… Understand the concept of dot product of two matrices.   
âœ… Understand the concept of autoregressive language models.  
âœ… Understand the concept of attention computation.  
âœ… Understand the workings of `Byte-Pair Encoding` (BPE) algorithm and tokenizer.   
âœ… Read and implement the workings of the `SentencePiece` library and tokenizer.  
âœ… Understand the concept of tokenization, input ids and embedding vectors.  
âœ… Understand & implement the concept of positional encoding.  
âœ… Understand the concept of single head self-attention.  
âœ… Understand the concept of scaled dot-product attention.  
âœ… Understand & implement the concept of multi-head attention.  
âœ… Understand & implement the concept of layer normalization.  
âœ… Understand the concept of masked multi-head attention & softmax layer.  
âœ… Understand and implement the concept of RMSNorm and difference with LayerNorm.  
âœ… Understand the concept of internal covariate shift.  
âœ… Understand the concept and implementation of feed-forward network with ReLU activation.  
âœ… Understand the concept and implementation of feed-forward network with SwiGLU activation.  
â˜‘ï¸ Understand the concept of absolute positional encoding.  
âœ… Understand the concept of relative positional encoding.  
â˜‘ï¸ Understand and implement the rotary positional embedding.   
â˜‘ï¸ Understand and implement grouped query attention from scratch.  
â˜‘ï¸ Understand and implement the concept of KV cache.   
â˜‘ï¸ Adding ...


### **Related GitHub Works:**
ğŸŒ [pytorch-llama](https://github.com/hkproj/pytorch-llama/tree/main) - PyTorch implementation of LLaMA by Umar Jamil.  
ğŸŒ [pytorch-transformer](https://github.com/hkproj/pytorch-transformer/tree/main) - PyTorch implementation of Transformer by Umar Jamil.  
ğŸŒ [llama](https://github.com/facebookresearch/llama) - Facebook's LLaMA implementation.  
ğŸŒ [tensor2tensor](https://github.com/tensorflow/tensor2tensor) - Google's transformer implementation.  
ğŸŒ [rmsnorm](https://github.com/bzhangGo/rmsnorm) - RMSNorm implementation.  
ğŸŒ [roformer](https://github.com/ZhuiyiTechnology/roformer) - Rotary Tranformer implementation.


### **Articles:**
âœ… [Understanding SentencePiece ([Under][Standing][_Sentence][Piece])](https://colabdoge.medium.com/understanding-sentencepiece-under-standing-sentence-piece-ac8da59f6b08)  
âœ… [SwiGLU: GLU Variants Improve Transformer (2020)](https://kikaben.com/swiglu-2020/#:~:text=The%20FFN%20with%20GELU%20activation%20becomes%3A%20FFN%20GELU,cumulative%20distribution%20function%20of%20the%20standard%20normal%20distribution.)