# **LLaMA**
![](llama.png)

### **Papers**  
I am reading these papers:
- [ ] [LLaMA: Open and Efficient Foundation Language Models](https://ai.meta.com/research/publications/llama-open-and-efficient-foundation-language-models/)
- [ ] [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)

### **Articles**
- [ ] [Understanding SentencePiece ([Under][Standing][_Sentence][Piece])](https://colabdoge.medium.com/understanding-sentencepiece-under-standing-sentence-piece-ac8da59f6b08)

### **Goals**
- [ ] Understand and implement grouped query attention from scratch.
- [ ] Understand and implement the concept of KV cache.
- [ ] Understand and implement the rotatory positional encoding.
- [ ] Adding ...
