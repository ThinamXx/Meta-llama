# **LLaMA**
![](llama.png)


### **PapersğŸ“„**  
I am reading these papers:  
âœ… [LLaMA: Open and Efficient Foundation Language Models](https://ai.meta.com/research/publications/llama-open-and-efficient-foundation-language-models/)  
â˜‘ï¸ [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)  
â˜‘ï¸ [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068)  
âœ… [Attention Is All You Need](https://arxiv.org/abs/1706.03762)  
âœ… [Root Mean Square Layer Normalization](https://arxiv.org/abs/1910.07467)  
âœ… [GLU Variants Improve Transformer](https://arxiv.org/abs/2002.05202)  
âœ… [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)  
âœ… [Self-Attention with Relative Position Representations](https://arxiv.org/pdf/1803.02155.pdf)  
â˜‘ï¸ [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)  
â˜‘ï¸ [To Fold or Not to Fold: a Necessary and Sufficient Condition on Batch-Normalization Layers Folding](https://arxiv.org/abs/2203.14646)  
âœ… [Fast Transformer Decoding: One Write-Head is All You Need](https://arxiv.org/abs/1911.02150)  
âœ… [GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](https://arxiv.org/abs/2305.13245)  
â˜‘ï¸ [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/abs/2204.02311)


### **Goals ğŸš€**
âœ… Understand the concept of dot product of two matrices.   
âœ… Understand the concept of autoregressive language models.  
âœ… Understand the concept of attention computation.  
âœ… Understand the workings of `Byte-Pair Encoding` (BPE) algorithm and tokenizer.   
âœ… Read and implement the workings of the `SentencePiece` library and tokenizer.  
âœ… Understand the concept of tokenization, input ids and embedding vectors.  
âœ… Understand & implement the concept of positional encoding.  
âœ… Understand the concept of single head self-attention.  
âœ… Understand the concept of scaled dot-product attention.  
âœ… Understand & implement the concept of multi-head attention.  
âœ… Understand & implement the concept of layer normalization.  
âœ… Understand the concept of masked multi-head attention & softmax layer.  
âœ… Understand and implement the concept of RMSNorm and difference with LayerNorm.  
âœ… Understand the concept of internal covariate shift.  
âœ… Understand the concept and implementation of feed-forward network with ReLU activation.  
âœ… Understand the concept and implementation of feed-forward network with SwiGLU activation.  
âœ… Understand the concept of absolute positional encoding.  
âœ… Understand the concept of relative positional encoding.  
âœ… Understand and implement the rotary positional embedding.  
âœ… Understand and implement the transformer architecture.  
âœ… Understand and implement the original Llama (1) architecture.   
âœ… Understand the concept of multi-query attention with single KV projection.   
âœ… Understand and implement grouped query attention from scratch.  
âœ… Understand and implement the concept of KV cache.  
âœ… Understand and implement the concept of Llama2 architecture.  
â˜‘ï¸ Download the checkpoints of Llama2 and inspect the inference code and working.  
â˜‘ï¸ Adding ...


### **Blog Posts:**  
âœ… [LLAMA: OPEN AND EFFICIENT LLM NOTES](https://thinamxx.github.io/thinam.github.io/posts/Llama1/llama1.html)  


### **Related GitHub Works:**
ğŸŒ [pytorch-llama](https://github.com/hkproj/pytorch-llama/tree/main) - PyTorch implementation of LLaMA by Umar Jamil.  
ğŸŒ [pytorch-transformer](https://github.com/hkproj/pytorch-transformer/tree/main) - PyTorch implementation of Transformer by Umar Jamil.  
ğŸŒ [llama](https://github.com/facebookresearch/llama) - Facebook's LLaMA implementation.  
ğŸŒ [tensor2tensor](https://github.com/tensorflow/tensor2tensor) - Google's transformer implementation.  
ğŸŒ [rmsnorm](https://github.com/bzhangGo/rmsnorm) - RMSNorm implementation.  
ğŸŒ [roformer](https://github.com/ZhuiyiTechnology/roformer) - Rotary Tranformer implementation.  
ğŸŒ [xformers](https://github.com/facebookresearch/xformers) - Facebook's implementation.


### **Articles:**
âœ… [Understanding SentencePiece ([Under][Standing][_Sentence][Piece])](https://colabdoge.medium.com/understanding-sentencepiece-under-standing-sentence-piece-ac8da59f6b08)  
âœ… [SwiGLU: GLU Variants Improve Transformer (2020)](https://kikaben.com/swiglu-2020/#:~:text=The%20FFN%20with%20GELU%20activation%20becomes%3A%20FFN%20GELU,cumulative%20distribution%20function%20of%20the%20standard%20normal%20distribution.)