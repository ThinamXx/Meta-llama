# **LLaMA**
![](llama.png)


### **PapersğŸ“„**  
I am reading these papers:  
â˜‘ï¸ [LLaMA: Open and Efficient Foundation Language Models](https://ai.meta.com/research/publications/llama-open-and-efficient-foundation-language-models/)  
â˜‘ï¸ [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)  
âœ… [Attention Is All You Need](https://arxiv.org/abs/1706.03762)  
â˜‘ï¸ [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068)  
â˜‘ï¸ [Root Mean Square Layer Normalization](https://arxiv.org/abs/1910.07467)


### **Goals ğŸš€**
âœ… Understand the concept of dot product of two matrices.   
âœ… Understand the concept of autoregressive language models.  
âœ… Understand the concept of attention computation.  
âœ… Understand the workings of `Byte-Pair Encoding` (BPE) algorithm and tokenizer.   
âœ… Read and implement the workings of the `SentencePiece` library and tokenizer.  
âœ… Understand the concept of tokenization, input ids and embedding vectors.  
âœ… Understand & implement the concept of positional encoding.  
âœ… Understand the concept of single head self-attention.  
âœ… Understand the concept of scaled dot-product attention.  
âœ… Understand & implement the concept of multi-head attention.  
âœ… Understand & implement the concept of layer normalization.  
âœ… Understand the concept of masked multi-head attention & softmax layer.  
â˜‘ï¸ Understand the concept of RMSNorm and difference with LayerNorm.  
â˜‘ï¸ Understand and implement the rotatory positional encoding.   
â˜‘ï¸ Understand and implement grouped query attention from scratch.  
â˜‘ï¸ Understand and implement the concept of KV cache.   
â˜‘ï¸ Adding ...


### **Related GitHub Works:**
ğŸŒ [pytorch-llama](https://github.com/hkproj/pytorch-llama/tree/main) - PyTorch implementation of LLaMA by Umar Jamil.  
ğŸŒ [pytorch-transformer](https://github.com/hkproj/pytorch-transformer/tree/main) - PyTorch implementation of Transformer by Umar Jamil.  
ğŸŒ [llama](https://github.com/facebookresearch/llama) - Facebook's LLaMA implementation.  
ğŸŒ [tensor2tensor](https://github.com/tensorflow/tensor2tensor) - Google's transformer implementation.  
ğŸŒ [rmsnorm](https://github.com/bzhangGo/rmsnorm) - RMSNorm implementation.


### **Articles:**
âœ… [Understanding SentencePiece ([Under][Standing][_Sentence][Piece])](https://colabdoge.medium.com/understanding-sentencepiece-under-standing-sentence-piece-ac8da59f6b08)