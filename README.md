# **LLaMA**
![](llama.png)


### **Papers📄**  
I am reading these papers:  
☑️ [LLaMA: Open and Efficient Foundation Language Models](https://ai.meta.com/research/publications/llama-open-and-efficient-foundation-language-models/)  
☑️ [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)  
✅ [Attention Is All You Need](https://arxiv.org/abs/1706.03762)  
☑️ [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068)  
☑️ [Root Mean Square Layer Normalization](https://arxiv.org/abs/1910.07467)


### **Goals 🚀**
✅ Understand the concept of dot product of two matrices.   
✅ Understand the concept of autoregressive language models.  
✅ Understand the concept of attention computation.  
✅ Understand the workings of `Byte-Pair Encoding` (BPE) algorithm and tokenizer.   
✅ Read and implement the workings of the `SentencePiece` library and tokenizer.  
✅ Understand the concept of tokenization, input ids and embedding vectors.  
✅ Understand & implement the concept of positional encoding.  
✅ Understand the concept of single head self-attention.  
✅ Understand the concept of scaled dot-product attention.  
✅ Understand & implement the concept of multi-head attention.  
✅ Understand & implement the concept of layer normalization.  
✅ Understand the concept of masked multi-head attention & softmax layer.  
☑️ Understand the concept of RMSNorm and difference with LayerNorm.  
☑️ Understand and implement the rotatory positional encoding.   
☑️ Understand and implement grouped query attention from scratch.  
☑️ Understand and implement the concept of KV cache.   
☑️ Adding ...


### **Related GitHub Works:**
🌐 [pytorch-llama](https://github.com/hkproj/pytorch-llama/tree/main) - PyTorch implementation of LLaMA by Umar Jamil.  
🌐 [pytorch-transformer](https://github.com/hkproj/pytorch-transformer/tree/main) - PyTorch implementation of Transformer by Umar Jamil.  
🌐 [llama](https://github.com/facebookresearch/llama) - Facebook's LLaMA implementation.  
🌐 [tensor2tensor](https://github.com/tensorflow/tensor2tensor) - Google's transformer implementation.  
🌐 [rmsnorm](https://github.com/bzhangGo/rmsnorm) - RMSNorm implementation.


### **Articles:**
✅ [Understanding SentencePiece ([Under][Standing][_Sentence][Piece])](https://colabdoge.medium.com/understanding-sentencepiece-under-standing-sentence-piece-ac8da59f6b08)